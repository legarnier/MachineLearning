{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook to test different weight initialization methods"
      ],
      "metadata": {
        "id": "zhONxL9MARLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38HaGVzv8Z8Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWLY2y7bvRR5"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2wCc97o8dJ6",
        "outputId": "325c1bb7-0613-41e8-beeb-045e78bec5e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 74703575.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9HqavIMNhJ1"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_res1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, stride=stride, bias=False)\n",
        "        self.conv_res1_bn = nn.BatchNorm2d(num_features=out_channels, momentum=0.9)\n",
        "        self.conv_res2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, bias=False)\n",
        "        self.conv_res2_bn = nn.BatchNorm2d(num_features=out_channels, momentum=0.9)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.relu(self.conv_res1_bn(self.conv_res1(x)))\n",
        "        out = self.conv_res2_bn(self.conv_res2(out))\n",
        "        out = self.relu(out)\n",
        "        out = residual + out\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A Residual network.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            #size (3,32,32)\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            #size (64,32,32)\n",
        "            nn.BatchNorm2d(num_features=64, momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #size (64,32,32)\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128, momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #size (128,32,32)\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #size (128,16,16)\n",
        "            ResidualBlock(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            #size (128,16,16)\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            #size (256,16,16)\n",
        "            nn.BatchNorm2d(num_features=256, momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #size (256,8,8)\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=256, momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #size (256,4,4)\n",
        "            ResidualBlock(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "            #size (256,4,4)\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #size (256,2,2)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(in_features=1024, out_features=10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = out.view(-1, out.shape[1] * out.shape[2] * out.shape[3])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zsLtoGpKZDm"
      },
      "outputs": [],
      "source": [
        "#Removed normalization layers\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_res1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, stride=stride, bias=False)\n",
        "        self.conv_res2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.relu(self.conv_res1(x))\n",
        "        out = self.conv_res2(out)\n",
        "        out = self.relu(out)\n",
        "        out = residual + out\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A Residual network.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            #size (3,32,32)\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            #size (64,32,32)\n",
        "            nn.ReLU(inplace=True),\n",
        "            #size (64,32,32)\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #size (128,32,32)\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #size (128,16,16)\n",
        "            ResidualBlock(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            #size (128,16,16)\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            #size (256,16,16)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #size (256,8,8)\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #size (256,4,4)\n",
        "            ResidualBlock(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "            #size (256,4,4)\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #size (256,2,2)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(in_features=1024, out_features=10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = out.view(-1, out.shape[1] * out.shape[2] * out.shape[3])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTC4gPlh8rOh"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJXPlPC5oeas"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def get_accuracy(model,loader,dataset,criterion):\n",
        "  with torch.no_grad():\n",
        "    pred = []\n",
        "    all_labels = []\n",
        "    losses = []\n",
        "    for data in loader:\n",
        "      inputs, labels = data\n",
        "      if torch.cuda.is_available():\n",
        "        inputs = inputs.to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "      outputs = model(inputs)\n",
        "      losses.append(criterion(outputs,labels).item())\n",
        "      batch_pred = torch.argmax(outputs,dim=1)\n",
        "      pred.append(batch_pred.detach())\n",
        "      all_labels.append(labels.detach())\n",
        "      del outputs\n",
        "      del batch_pred\n",
        "      del inputs\n",
        "      del labels\n",
        "\n",
        "\n",
        "    pred = torch.concat(pred).to('cpu')\n",
        "    all_labels = torch.concat(all_labels).to('cpu')\n",
        "    loss = np.mean(losses)\n",
        "    accuracy = ((all_labels==pred).sum()/len(all_labels)).item()\n",
        "    return accuracy,loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5Pd5VQRjz_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def init_network_kaiming_in(gain=1):\n",
        "  torch.manual_seed(0)\n",
        "  resnet = ResNet()\n",
        "\n",
        "  conv_layers = [resnet.conv[0],resnet.conv[3],resnet.conv[7].conv_res1,resnet.conv[7].conv_res2,\n",
        "                resnet.conv[8],resnet.conv[12],resnet.conv[16].conv_res1,resnet.conv[16].conv_res2,\n",
        "                ]\n",
        "\n",
        "\n",
        "  # Initialization strategy\n",
        "\n",
        "  for layer in conv_layers:\n",
        "    nn.init.kaiming_uniform_(layer.weight,mode='fan_in',nonlinearity='relu')\n",
        "    layer.weight.data = layer.weight.detach()/math.sqrt(6)\n",
        "    layer.weight.data = layer.weight.detach()*gain\n",
        "  return resnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from scipy.linalg import solve_sylvester\n",
        "import numpy as np\n",
        "\n",
        "# Sylvester Data driven initialization\n",
        "# Based on Das et al. (2021)\n",
        "def sylvester_init(inputs,c_out,kernel_size=3,alpha=10,latent_representation = 'PCA'):\n",
        "  \"\"\"\n",
        "  inputs : tensor of size (n,c_in,im_size,im_size)\n",
        "  c_out : # of output channels\n",
        "  alpha : Weight of the encoding error, see Das et al. paper\n",
        "  \"\"\"\n",
        "  n,c_in,im_size,_ = inputs.size()\n",
        "  #Randomly picks parts of pictures in the inputs\n",
        "  n_p = int(3e3)\n",
        "  X = torch.tensor([])\n",
        "  for k in range(n_p):\n",
        "    image = np.random.randint(n)\n",
        "    point = np.random.randint(im_size-kernel_size+1)\n",
        "    part = inputs.detach()[image,:,point:point+kernel_size,point:point+kernel_size].unsqueeze(0)\n",
        "    X = torch.cat([X,part])\n",
        "  # From size (n_p,c_in,kernel_size,kernel_size) to (n_p,c_in*kernel_size*kernel_size)\n",
        "  X = X.view((-1,c_in*kernel_size*kernel_size))\n",
        "\n",
        "  if latent_representation == 'PCA':\n",
        "    pca = PCA(c_out)\n",
        "    S = torch.tensor(pca.fit_transform(X),dtype=torch.float)\n",
        "  else:\n",
        "    # Other methods are possible\n",
        "    return\n",
        "  A = S.T@S #size (c_out,c_out)\n",
        "  A = A.numpy()\n",
        "  B = alpha * X.T@X #size (c_in*kernel_size*kernel_size,c_in*kernel_size*kernel_size)\n",
        "  B = B.numpy()\n",
        "  C = (1+alpha) * S.T@X #size (c_out,c_in*kernel_size*kernel_size)\n",
        "  C = C.numpy()\n",
        "\n",
        "  # Solve equation : AW + WB = C\n",
        "  W = solve_sylvester(A,B,C)\n",
        "  W = torch.tensor(W).view(c_out,c_in,kernel_size,kernel_size)\n",
        "  return W"
      ],
      "metadata": {
        "id": "IRV8_Fh7wZZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def init_network_sylvester(alpha=10):\n",
        "  torch.manual_seed(0)\n",
        "  np.random.seed(0)\n",
        "  resnet = ResNet()\n",
        "\n",
        "  # Use this when batch normalization layers are used\n",
        "  # conv_layers = [resnet.conv[0],resnet.conv[3],resnet.conv[7].conv_res1,resnet.conv[7].conv_res2,\n",
        "  #               resnet.conv[8],resnet.conv[12],resnet.conv[16].conv_res1,resnet.conv[16].conv_res2,\n",
        "  #               ]\n",
        "  # Use this when batch normalization layers are not used\n",
        "  conv_layers = [resnet.conv[0],resnet.conv[2],resnet.conv[5].conv_res1,resnet.conv[5].conv_res2,\n",
        "                resnet.conv[6],resnet.conv[9],resnet.conv[12].conv_res1,resnet.conv[12].conv_res2,\n",
        "                ]\n",
        "\n",
        "  # Initialization strategy\n",
        "  inputs=torch.tensor([])\n",
        "  for i,batch in enumerate(trainloader):\n",
        "    inputs = torch.cat([inputs,batch[0]])\n",
        "    break\n",
        "\n",
        "\n",
        "  for i,layer in enumerate(tqdm(conv_layers)):\n",
        "    # c_out/c_in too large for first layer\n",
        "    if i > 0:\n",
        "      c_out = layer.weight.size(0)\n",
        "      W = sylvester_init(inputs,c_out,alpha=alpha)\n",
        "      layer.weight.data = W.detach()\n",
        "    inputs = layer(inputs)\n",
        "  return resnet"
      ],
      "metadata": {
        "id": "nwMH38UB1gt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = ResNet()\n",
        "resnet.conv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SGcTffh6dBz",
        "outputId": "44ffe42f-8773-4a9b-ceee-741a026244b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): ResidualBlock(\n",
              "    (conv_res1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv_res2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (7): ReLU(inplace=True)\n",
              "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (10): ReLU(inplace=True)\n",
              "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (12): ResidualBlock(\n",
              "    (conv_res1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv_res2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KzGmqfdN7-y"
      },
      "outputs": [],
      "source": [
        "def train_model(model,n_epoch=50,verbose=True):\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    resnet.to('cuda')\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(resnet.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "  memory_running_loss = []\n",
        "  memory_train_loss = []\n",
        "  memory_test_loss = []\n",
        "  memory_train_accuracy = []\n",
        "  memory_test_accuracy = []\n",
        "  for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "          if torch.cuda.is_available():\n",
        "            inputs = inputs.to('cuda')\n",
        "            labels = labels.to('cuda')\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = resnet(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 49 == 48:    # print every 30 mini-batches\n",
        "              if verbose>1:\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 49:.3f}')\n",
        "              with torch.no_grad():\n",
        "                memory_running_loss.append(running_loss)\n",
        "              running_loss = 0.0\n",
        "      with torch.no_grad():\n",
        "        train_accuracy,train_loss = get_accuracy(resnet,trainloader,trainset,criterion)\n",
        "        memory_train_accuracy.append(train_accuracy)\n",
        "        memory_train_loss.append(train_loss)\n",
        "        test_accuracy,test_loss = get_accuracy(resnet,testloader,testset,criterion)\n",
        "        memory_test_accuracy.append(test_accuracy)\n",
        "        memory_test_loss.append(test_loss)\n",
        "        if verbose:\n",
        "          print(f\"\\nepoch : {epoch+1}, train accuracy : {train_accuracy}, test accuracy : {test_accuracy}\\n\")\n",
        "          print(f\"train loss : {train_loss}, test loss : {test_loss}\\n\")\n",
        "  return memory_running_loss,memory_train_accuracy,memory_test_accuracy,memory_train_loss,memory_test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FKjzhe80n0v"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "tested_gains = [1/100,1/10,1/math.sqrt(6),1/math.sqrt(3),1,math.sqrt(3),math.sqrt(6),10,100]\n",
        "\n",
        "for gain in tqdm(tested_gains):\n",
        "  resnet = init_network_kaiming_in(gain=gain)\n",
        "  memory_running_loss,memory_train_accuracy,memory_test_accuracy,memory_train_loss,memory_test_loss = train_model(resnet,n_epoch=20,verbose=1)\n",
        "\n",
        "  np.save(f\"./results2/running_loss_{gain:.3f}\",memory_running_loss)\n",
        "  np.save(f\"./results2/train_accuracy_{gain:.3f}\",memory_train_accuracy)\n",
        "  np.save(f\"./results2/test_accuracy_{gain:.3f}\",memory_test_accuracy)\n",
        "  np.save(f\"./results2/train_loss_{gain:.3f}\",memory_train_loss)\n",
        "  np.save(f\"./results2/test_loss_{gain:.3f}\",memory_test_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91Hj2ayRn9sH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "698841c6-4bfc-4321-ae6f-8e706ea4caf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/outputs saves 2.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"./outputs saves 2\", 'zip', './results2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [0.1,1,10]:\n",
        "  resnet = init_network_sylvester(alpha=alpha)\n",
        "  memory_running_loss,memory_train_accuracy,memory_test_accuracy,memory_train_loss,memory_test_loss = train_model(resnet,n_epoch=20,verbose=1)\n",
        "  np.save(f\"./results3/running_loss_{alpha:.1f}\",memory_running_loss)\n",
        "  np.save(f\"./results3/train_accuracy_{alpha:.1f}\",memory_train_accuracy)\n",
        "  np.save(f\"./results3/test_accuracy_{alpha:.1f}\",memory_test_accuracy)\n",
        "  np.save(f\"./results3/train_loss_{alpha:.1f}\",memory_train_loss)\n",
        "  np.save(f\"./results3/test_loss_{alpha:.1f}\",memory_test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z5MP58r5OvIC",
        "outputId": "f6bdb916-015c-470d-ed74-ee341d84ea19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [02:43<00:00, 20.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch : 1, train accuracy : 0.3946399986743927, test accuracy : 0.39800000190734863\n",
            "\n",
            "train loss : 1.683637232804785, test loss : 1.6699026644229888\n",
            "\n",
            "\n",
            "epoch : 2, train accuracy : 0.5346400141716003, test accuracy : 0.5200999975204468\n",
            "\n",
            "train loss : 1.3310637352417927, test loss : 1.3460669189691543\n",
            "\n",
            "\n",
            "epoch : 3, train accuracy : 0.6069999933242798, test accuracy : 0.588100016117096\n",
            "\n",
            "train loss : 1.114758890806412, test loss : 1.162567573785782\n",
            "\n",
            "\n",
            "epoch : 4, train accuracy : 0.6823400259017944, test accuracy : 0.6448000073432922\n",
            "\n",
            "train loss : 0.9008758363066888, test loss : 1.001242396235466\n",
            "\n",
            "\n",
            "epoch : 5, train accuracy : 0.7673599720001221, test accuracy : 0.704200029373169\n",
            "\n",
            "train loss : 0.6807498782873154, test loss : 0.8473420888185501\n",
            "\n",
            "\n",
            "epoch : 6, train accuracy : 0.8090400099754333, test accuracy : 0.7335000038146973\n",
            "\n",
            "train loss : 0.5620736262323905, test loss : 0.7870441183447838\n",
            "\n",
            "\n",
            "epoch : 7, train accuracy : 0.8418400287628174, test accuracy : 0.7439000010490417\n",
            "\n",
            "train loss : 0.46804795900777896, test loss : 0.765421350300312\n",
            "\n",
            "\n",
            "epoch : 8, train accuracy : 0.8637800216674805, test accuracy : 0.7458000183105469\n",
            "\n",
            "train loss : 0.3914512818565174, test loss : 0.7939571805298329\n",
            "\n",
            "\n",
            "epoch : 9, train accuracy : 0.8883799910545349, test accuracy : 0.7379999756813049\n",
            "\n",
            "train loss : 0.32720879016786203, test loss : 0.8303768321871757\n",
            "\n",
            "\n",
            "epoch : 10, train accuracy : 0.9166399836540222, test accuracy : 0.7436000108718872\n",
            "\n",
            "train loss : 0.23864953843306522, test loss : 0.8877450436353683\n",
            "\n",
            "\n",
            "epoch : 11, train accuracy : 0.9494600296020508, test accuracy : 0.7512999773025513\n",
            "\n",
            "train loss : 0.15776377851713677, test loss : 0.9012259043753147\n",
            "\n",
            "\n",
            "epoch : 12, train accuracy : 0.9639000296592712, test accuracy : 0.7538999915122986\n",
            "\n",
            "train loss : 0.11161901165104034, test loss : 0.9816540494561196\n",
            "\n",
            "\n",
            "epoch : 13, train accuracy : 0.9726999998092651, test accuracy : 0.7620000243186951\n",
            "\n",
            "train loss : 0.08303210285625287, test loss : 1.0615704938769341\n",
            "\n",
            "\n",
            "epoch : 14, train accuracy : 0.9712200164794922, test accuracy : 0.7595000267028809\n",
            "\n",
            "train loss : 0.0909818584783649, test loss : 1.0905428498983383\n",
            "\n",
            "\n",
            "epoch : 15, train accuracy : 0.9791799783706665, test accuracy : 0.753000020980835\n",
            "\n",
            "train loss : 0.06434893045498401, test loss : 1.1916082069277762\n",
            "\n",
            "\n",
            "epoch : 16, train accuracy : 0.9769600033760071, test accuracy : 0.7623999714851379\n",
            "\n",
            "train loss : 0.06863332171069116, test loss : 1.2254872113466262\n",
            "\n",
            "\n",
            "epoch : 17, train accuracy : 0.9687600135803223, test accuracy : 0.7437999844551086\n",
            "\n",
            "train loss : 0.08977938604978275, test loss : 1.4311809226870538\n",
            "\n",
            "\n",
            "epoch : 18, train accuracy : 0.9721400141716003, test accuracy : 0.7523999810218811\n",
            "\n",
            "train loss : 0.08412202734652223, test loss : 1.4120746493339538\n",
            "\n",
            "\n",
            "epoch : 19, train accuracy : 0.9799399971961975, test accuracy : 0.7609000205993652\n",
            "\n",
            "train loss : 0.05835194549314222, test loss : 1.359398140013218\n",
            "\n",
            "\n",
            "epoch : 20, train accuracy : 0.9842399954795837, test accuracy : 0.755299985408783\n",
            "\n",
            "train loss : 0.04504370060986934, test loss : 1.4379685282707215\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-371517c138b0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_network_sylvester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmemory_running_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_train_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_test_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_train_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./results3/running_loss_{alpha:.1f}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_running_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./results3/train_accuracy_{alpha:.1f}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_train_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./results3/test_accuracy_{alpha:.1f}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_test_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results3/running_loss_0.1.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial network (before changes from sylvester)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "resnet = ResNet()\n",
        "memory_running_loss,memory_train_accuracy,memory_test_accuracy,memory_train_loss,memory_test_loss = train_model(resnet,n_epoch=20,verbose=1)\n",
        "np.save(f\"./results3/running_loss_init\",memory_running_loss)\n",
        "np.save(f\"./results3/train_accuracy_init\",memory_train_accuracy)\n",
        "np.save(f\"./results3/test_accuracy_init\",memory_test_accuracy)\n",
        "np.save(f\"./results3/train_loss_init\",memory_train_loss)\n",
        "np.save(f\"./results3/test_loss_init\",memory_test_loss)"
      ],
      "metadata": {
        "id": "lOLVtSY-YdI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PdCtnLaY0Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"./outputs saves 3\", 'zip', './results3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ptarmkd4Sdfj",
        "outputId": "7b5deedb-ec6e-48be-dd2a-b6c7c1c09a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/outputs saves 3.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eA4Cl4iSe7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}